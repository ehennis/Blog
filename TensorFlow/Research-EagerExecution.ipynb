{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eager Execution Basics and Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the first [tutorial](https://www.tensorflow.org/tutorials/eager/eager_basics) in the research and experimentation section of the [tutorials](https://www.tensorflow.org/tutorials/eager/). This is now standard with version 2 of TensorFlow but I can still learn something."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tensors**  \n",
    "Up until recently, I didn't know what 'tensor' was in TensorFlow. I just assumed it was something ML related and moved on. Turns out, it is a multi-dimensional array. I used something similar in numpy with their ndarray mutliple times across multiple classes. Tensors can reside in GPU (I wish crypto would crash so I could pick one up at a normal prices) memory.  \n",
    "\n",
    "The differences between tensors and ndarrays:  \n",
    "* Tensors can be backed by accelerator memory (GPU/TPU)  \n",
    "* Tensors are immutable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(3, shape=(), dtype=int32)\n",
      "tf.Tensor([4 5], shape=(2,), dtype=int32)\n",
      "tf.Tensor(25, shape=(), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "tf.Tensor(b'aGVsbG8gd29ybGQ', shape=(), dtype=string)\n",
      "tf.Tensor(13, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "#Built in methods\n",
    "print(tf.add(1,2)) #Notice the type returned. You have the value but also a shape and data type\n",
    "print(tf.add([1,2],[3.4]))\n",
    "print(tf.square(5))\n",
    "print(tf.reduce_sum([1,2,3]))\n",
    "print(tf.encode_base64(\"hello world\"))\n",
    "#Operator overloading is also supported\n",
    "print(tf.square(2) + tf.square(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)\n",
      "<dtype: 'int32'>\n"
     ]
    }
   ],
   "source": [
    "x = tf.matmul([[1]],[[2,3]])\n",
    "print(x.shape)\n",
    "print(x.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conversion between NumPy and Tensors**  \n",
    "TensorFlow operations automatically convert NumPy ndarrays to Tensors.  \n",
    "NumPy operations automatically convert Tensors to NumPy ndarrays.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow operations convert numpy arrays to Tensors automatically\n",
      "tf.Tensor(\n",
      "[[42. 42. 42.]\n",
      " [42. 42. 42.]\n",
      " [42. 42. 42.]], shape=(3, 3), dtype=float64)\n",
      "And NumPy operations convert Tensors to numpy arrays automatically\n",
      "[[43. 43. 43.]\n",
      " [43. 43. 43.]\n",
      " [43. 43. 43.]]\n",
      "The .numpy() method explicitly converts a Tensor to a numpy array\n",
      "[[42. 42. 42.]\n",
      " [42. 42. 42.]\n",
      " [42. 42. 42.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "ndarray = np.ones([3,3])\n",
    "print(\"TensorFlow operations convert numpy arrays to Tensors automatically\")\n",
    "tensor = tf.multiply(ndarray,42)\n",
    "print(tensor)\n",
    "print(\"And NumPy operations convert Tensors to numpy arrays automatically\")\n",
    "print(np.add(tensor, 1))\n",
    "print(\"The .numpy() method explicitly converts a Tensor to a numpy array\")\n",
    "print(tensor.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GPU Acceleration**  \n",
    "I don't have a GPU to run on but if I did I could set the Tensor to execute on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is there a GPU available: \n",
      "False\n",
      "Is the Tensor on GPU #0:  \n",
      "False\n"
     ]
    }
   ],
   "source": [
    "#None of this works!!\n",
    "x = tf.random_uniform([3, 3])\n",
    "\n",
    "print(\"Is there a GPU available: \"),\n",
    "print(tf.test.is_gpu_available())\n",
    "\n",
    "print(\"Is the Tensor on GPU #0:  \"),\n",
    "print(x.device.endswith('GPU:0'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since I don't have a GPU to work with I am going to skip the section working with them since I can't show the speed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets  \n",
    "I have always loaded data from a CSV using Pandas or using the built in ones. So, this section was all new to me."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Eager vs Graph**  \n",
    "When creating the data set without eager execution is the same. The difference some when you are iterating over the elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ds_tensors = tf.data.Dataset.from_tensor_slices([1,2,3,4,5,6])\n",
    "#Create CSV\n",
    "import tempfile\n",
    "_, filename = tempfile.mkstemp()\n",
    "\n",
    "with open(filename, 'w') as f:\n",
    "    f.write(\"\"\"Line 1\n",
    "Line 2\n",
    "Line 3\n",
    "    \"\"\")\n",
    "    \n",
    "ds_file = tf.data.TextLineDataset(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Transformations/Iterations**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ds_tensors = ds_tensors.map(tf.square).shuffle(2).batch(2)\n",
    "ds_file = ds_file.batch(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elemens of ds_tensors\n",
      "tf.Tensor([1 9], shape=(2,), dtype=int32)\n",
      "tf.Tensor([ 4 25], shape=(2,), dtype=int32)\n",
      "tf.Tensor([36 16], shape=(2,), dtype=int32)\n",
      "\n",
      "Elements in ds_file\n",
      "tf.Tensor([b'Line 1' b'Line 2'], shape=(2,), dtype=string)\n",
      "tf.Tensor([b'Line 3' b'    '], shape=(2,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "print('Elemens of ds_tensors')\n",
    "for x in ds_tensors:\n",
    "    print(x)\n",
    "\n",
    "print(\"\\nElements in ds_file\")\n",
    "for x in ds_file:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Tape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since these are short I am going to roll the [Automatic Differentiation](https://www.tensorflow.org/tutorials/eager/automatic_differentiation) into this same notebook.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow provides *tf.GradientType* for automatic differentiation (computing the gradient of a computation with respect to its input variables)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.ones((2,2))\n",
    "\n",
    "with tf.GradientTape() as t:\n",
    "    t.watch(x)\n",
    "    y = tf.reduce_sum(x) #4.0\n",
    "    z = tf.multiply(y,y) # 16.0\n",
    "\n",
    "#Derivative of z with respect to the original input tensor x\n",
    "dz_dx = t.gradient(z, x) # [[8,8],[8,8]]\n",
    "\n",
    "for i in [0,1]:\n",
    "    for j in [0,1]:\n",
    "        assert dz_dx[i][j].numpy() == 8.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1. 1.]\n",
      " [1. 1.]], shape=(2, 2), dtype=float32)\n",
      "tf.Tensor(4.0, shape=(), dtype=float32)\n",
      "tf.Tensor(16.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.ones((2,2))\n",
    "\n",
    "with tf.GradientTape() as t:\n",
    "    t.watch(x)\n",
    "    y = tf.reduce_sum(x)\n",
    "    z = tf.multiply(y,y)\n",
    "\n",
    "# Use the tape to compute the derivative of z w/r/t the intermediate value y\n",
    "dz_dy = t.gradient(z,y)\n",
    "assert dz_dy.numpy() == 8.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the resources are released as soon as the *gradient* method is called. To keep these around for multiple calls you need to set *persistent* property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.constant(3.0)\n",
    "with tf.GradientTape(persistent=True) as t:\n",
    "    t.watch(x)\n",
    "    y = x * x\n",
    "    z = y * y\n",
    "dz_dx = t.gradient(z, x) # 108 => (4 * x^3 as x = 3)\n",
    "dy_dx = t.gradient(y, x) # 6.0\n",
    "del t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recording control flow**  \n",
    "Since the tapes are recording the operations as they are executed, pyton control flow (if/while) is naturally handled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f(x, y):\n",
    "  output = 1.0\n",
    "  for i in range(y):\n",
    "    if i > 1 and i < 5:\n",
    "      output = tf.multiply(output, x)\n",
    "  return output\n",
    "\n",
    "def grad(x, y):\n",
    "  with tf.GradientTape() as t:\n",
    "    t.watch(x)\n",
    "    out = f(x, y)\n",
    "  return t.gradient(out, x)\n",
    "\n",
    "x = tf.convert_to_tensor(2.0)\n",
    "\n",
    "assert grad(x, 6).numpy() == 12.0\n",
    "assert grad(x, 5).numpy() == 12.0\n",
    "assert grad(x, 4).numpy() == 4.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Higher-order gradients**  \n",
    "Operations inside of *GradientTape* context manager are recorded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.Variable(1.0)  # Create a Tensorflow variable initialized to 1.0\n",
    "\n",
    "with tf.GradientTape() as t:\n",
    "  with tf.GradientTape() as t2:\n",
    "    y = x * x * x\n",
    "  # Compute the gradient inside the 't' context manager\n",
    "  # which means the gradient computation is differentiable as well.\n",
    "  dy_dx = t2.gradient(y, x)\n",
    "d2y_dx2 = t.gradient(dy_dx, x)\n",
    "\n",
    "assert dy_dx.numpy() == 3.0\n",
    "assert d2y_dx2.numpy() == 6.0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf_current]",
   "language": "python",
   "name": "conda-env-tf_current-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
